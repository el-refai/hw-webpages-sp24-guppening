<!DOCTYPE HTML>
<!--
	Hyperspace by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>CS 184 Final Project: LE4GS - Language Embedded 4D Gaussian Splatting</title>
    <link rel="icon" type="image/x-icon" href="/images/favicon.ico">
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Sidebar -->
			<section id="sidebar">
				<div class="inner">
					<nav>
						<ul>
							<li><a href="#intro">Abstract</a></li>
							<li><a href="#technical-approach">Technical Approach</a></li>
							<li><a href="#results">Results</a></li>
							<li><a href="#references">References</a></li>
							<li><a href="#contributions">Contributions</a></li>
						</ul>
					</nav>
				</div>
			</section>

		<!-- Wrapper -->
			<div id="wrapper">

				<section class="wrapper style1 fullscreen fade-up">
					<div class="inner">
						<h1>CS 184 Final Project: LE4GS 
              <br />
              Language Embedded 4D Gaussian Splatting
            </h1>
            <h2>
              James DeLoye, Karim El-Refai, Andrew Wang, Jenny Nguyen
            </h2>
					</div>
				</section>
				<!-- Intro -->
					<section id="intro" class="wrapper style1-alt fullscreen fade-up">
						<div class="inner">
							<h1>Abstract</h1>
							<p>
                Quick synthesis of novel views in a 3D scene given a limited amount of
                views is a fast-growing research area in graphics and computer vision. A
                variety of approaches to learn the plenoptic function have been
                explored, including Neural Radiance Fields, Plenoxels, and recently, 3D
                Gaussian Splatting, a technique where a machine learning model fills a
                scene with millions of anisotropic 3D Gaussians and optimizes their
                shape, size, and color to approximate the scene as best as possible.
              </p>
              <p>
                Extensions to Gaussian splatting such as 4D Gaussian Splatting and
                Language-Embedded 3D Gaussian Splatting allow for quick rendering of
                novel views within a dynamic scene from obtained from videos (as opposed
                a static scene obtained from images) as well as identification/semantic
                encoding of objects in a scene, respectively.
              </p>
              <p>
                Our project aims to combine the two to add semantic encoding across time
                within the changing scene learned by Gaussian Splatting, allowing the
                creation of heatmaps and fast object tracking within scenes.
              </p>
							<!-- <ul class="actions">
								<li><a href="#one" class="button scrolly">Learn more</a></li>
							</ul> -->
						</div>
					</section>

				<!-- One -->
					<section id="technical-approach" class="wrapper style2 fade-up">
								<div class="inner">
									<h2>Technical Approach - Background</h2>
                  <h3>Gaussian Splatting</h3>
                  <p>
                    Gaussian splatting is a rendering technique that is can represent 3 dimensional scenes by "painting" with 3D gaussians. It has quickly gained in popularity due it its ability to render complex scenes without the high computational cost of NERFs or other 3D rendering techniques. 3D renders based on scans of the real world have a number of techniques, ranging from autonomous vehicles, to video games, to movies, to the medical field.
                    The first step in generating a splat is to gather data, typically a series of photos or a video of a single object or scene taken with a RGB camera and some sort of depth sensor, such as LIDAR.
                    With many perspectives of the same scene, the underlying depth and 3D structure of the scene can be inferred using structure from motion (SFM) algorithms. 
                    Below is an example of a point cloud generated from an SFM algorithm.
                  </p>
                  <div style="display: flex; justify-content: center;">
                  <figure>
                    <img src="images/coli-cloud.jpg" alt="A point cloud of the Coliseum generated from SFM algorithms" style="height: 300px;">
                    <figcaption style="text-align: center;">A point cloud of the Coliseum generated from SFM algorithms<sup>9</sup></figcaption>
                  </figure>
                  </div>
                  <p>
                    To each point in this cloud, a 3D gaussian, the extension of the 1D gaussian (bell curve/normal distribution) to 3 dimensions, is assigned. Each one of these gaussians has a center, determined by the corresponding point in a point cloud, and a covariance matrix, which determines the shape of the gaussian. Note that these gaussians are anisotropic, meaning that the axes are not necessarily axis-aligned. Finally, each gaussian also has a color and an opacity. The values of all these attributes are determined by a neural network which is trained to minimize the difference between the image generated by the rendered gaussians and the ground truth data. Since gaussians are infinitely differentiable, these optimal values can be found via gradient descent. Below is a scene with a bicycle rendered with Gaussian Splatting, and the same scene with fully opaque gaussians such that each can be clearly distinguished. Finally there is a "densification" step, which removes gaussians that are so transparent that they don't contribute to the image, or divides gaussians that are so big that there is a significant loss in detail.
                  </p>
                  <div style="display: flex; justify-content: center;">
                  <figure>
                    <img src="images/bicycle.png" alt="A bike rendered with gaussian splatting" style="padding-right:10px; height: 300px;"/>
                    <figcaption style="text-align: center;">A bike rendered with Gaussian Splatting<sup>10</sup></figcaption>
                  </figure>
                  <figure>
                    <img src="images/opaque.png" alt="A bike with fully opaque gaussians" style="height: 300px;"/>
                    <figcaption style="text-align: center;">The same bike Gaussian Splat with fully opaque gaussians<sup>10</sup> </figcaption>
                  </figure>
                  </div>          
                  <div style="display: flex; justify-content: center;">          
                  <video width="80%" controls>
                    <source src="images/bicycle.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                  </div>
                  <div style="text-align: center;">The same bicycle dynamically rendered in 3D using Gaussian Splatting<sup>10</sup> </div>
                  <br>
                  <h3>Dynamic and Language Embedded Splatting</h3>
									<p>
                    As Gaussian Splatting has gained in popularity, the original technique has been iterated upon and developed in a number of ways. One such way has been to extend the technique to dynamic scenes, essentially allowing for the rendering of 3D video. This is a significant area of interest given its applications to areas like autonomous vehicles, robotics, and entertainement. This has been accomplished in a number of ways ranging from moving the gaussians throughout the scene to track movement to simply extending the gaussians to a 4th dimension, and allowing them to vary over time. We based our approach on the latter technique, working off a 2024 paper, Real-time Photorealistic Dynamic Scene Representation and Rendering with 4D Gaussian Splatting, which presents some of the best open source results. Below are two videos of scenes rendered using this technique.<sup>1</sup>
                  </p>
                  <div style="display: flex; justify-content: center;">          
                    <video width="40%" style="padding-right: 10px;" controls>
                      <source src="images/cookingguy.mp4" type="video/mp4">
                      Your browser does not support the video tag.
                    </video>
                    <video width="40%" controls>
                      <source src="images/cars.mp4" type="video/mp4">
                      Your browser does not support the video tag.
                    </video>
                    </div>
                    <br>
                    <p>
                      A second area of interest has been the addition of semantic encoding to 3D gaussian splats. This allows for computer-based object recognition within the scene and can be used in a number of applications, particularly those involving autonomous systems like robots or cars. It can also allow humans to look for objects in scenes using natural language processing to describe objects. One of the highest quality and most recent implementations of this idea has been in the paper LangSplat: 3D Language Gaussian Splatting. This paper gets embeddings for individual gaussians by first segmenting the scene into three levels of objects using Meta's Segment Anything model. Then, for each of these segments, the object recognition model CLIP is used to get an embedding for these areas. If multiple embeddings overlap the same gaussian, they are all applied to each gaussian. Finally, these embeddings are projected onto a lower dimensional space using an autoencoder to save memory, given that only a small subset of all possible things will ever exist in a single scene. These embeddings can then be rendered in the scene at different levels of specificity (object, parts, and subparts). In addition, because these embeddings are related to each gaussian, they can be used to generate heatmaps of the scene or reply to natural language queries. Below is an example of a scene processed using this technique.<sup>2</sup>
                    </p>

                    <div style="display: flex; justify-content: center;">          
                      <video width="80%" controls>
                        <source src="images/waldo.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video>
                      </div>
                      <br>
                    <p>
                      Our primary goal for this project was to, for the first time, combine these two techniques, allowing for dynamic (4D) Gaussian Splatting with semantic embeddings. This would allow for semantic and dymanic object tracking within scenes, once again highly desirable and relevant for real-time autonomous systems operating in the real world. Below, our technical approach is detailed. 
                    </p>
								</div>
					</section>

          <section class="wrapper style2-alt fade-up">
              <div class="inner">
                <h2>Technical Approach - Contributions</h2>
                <h3>Real-Time Dynamic and Language Embedded Gaussian Visualization</h3>
                <p>
                We continuously update the scene represntation based on incoming data streams and track it using timestamps for real-time visualizations of dynamic scenes. The models learns the dynamic scene using 4DGS and LEGS, which allow
				it to adapt to changes in the environment over time. The initializastion phase establishes a communication channel between the model and the SIBR viewer. The socket listens for updates from the viewer and takes the camera parameters to
				render in real time.
				</p>
    <h3>Steps for Rendering in the Viewer:</h3>
    <ol>
      <li>Model learns dynamic scene using 4DGS or 3DLEGS</li>
      <li>Initialization
        <ul>
          <li>Initialize a socket to establish communication with SIBR viewer through which the model and viewer communicate</li>
          <li>Two-way communication initiated between model client and viewer when both are running</li>
        </ul>
      </li>
      <li style="margin-top: -40px;">Rendering Loop
        <ul>
          <li>SIBR viewer sends camera parameters to model through established socket connection</li>
          <li>CUDA kernel renders a frame based on the parameters received through the socket and sends it back to the viewer</li>
        </ul>
      </li>
      <li style="margin-top: -40px;">Viewer Display
        <ul>
          <li>User changes parameters to affect visualization through interactive panels in the viewer (timestamp, language features, etc)</li>
        </ul>
      </li>
    </ol>
                <h3>Language Embedded 4D Gaussian Splatting</h3>
                <p>
                  Before diving into the project, we felt that combining the two components would be challenging, but relatively straightforward. We mistakenly assumed that the two models could be combined solely by modifying the underlying model parameters in the actual gaussian model itself and by modifying the training routine. However, after working with both repositories more, we realized that the approaches differed significantly from one another and would need to be integrated together on a nearly line-by-line basis for every file in the repository This required us to become intimately familiar with the codebase of both models, and become familiar with the CUDA used to render the scenes and train the models on the GPUs. Throughout this process we ran into a number of issues, particularly with CUDA errors. Since none of us had worked extensively with CUDA code before, learning how to debug these errors, especially those related to memory issues was a significant challenge. However, once we had the merged repository working, we were able to attempt to train some models. The process went as follows:
                  <ul>
                    <li> Train the 4DGS model on some dynamic scene: The 4DGS model is first trained on the scene normally without doing any embedding to represent a baseline to build the language embedded model off of. </li>
                    <li> Train the language embedded model on the same scene: The LEGS model is then trained on the same 4D scene as a whole to obtain the per-gaussian embeddings. </li>
                    <li> Finetune the 4DGS model with the embeddings: The embeddings are then added to the 4DGS model and the models is trained such that the semantic segments are clearly visible in the final image. </li>
                  </ul>
                  Though we were able to get these models to train and go through the whole pipeline, when we attempted to view the dynamic scenes with language embeddings in the SIBR viewer, it seemed that the embeddings were not properly propagated to each gaussian. As of now, we are unsure whether the issue is with the integration of the two models, or conflicting embeddigs for the same gaussian in differing timesteps. In the meantime, we are continuing to work on the project and hope to have a working model, but we are still pleased with the quality of the 4D renders that we have been able to produce.
                </p>
              </div>
          </section>

				<!-- Results -->
					<section id="results" class="wrapper style3 fade-up">
						<div class="inner">
							<h2>Results</h2>
              <div style="text-align: center;">     
                <h3>Final Project Video</h3>
                <video controls width="100%">
                  <source src="videos/LE Dynamic Gaussian Splatting Presentation-1.webm" type="video/webm">
                  Your browser does not support the video tag.
                </video>
              </div>

    <h3>4DGS Interactive Viewer Visualizations</h3>
      <div style="text-align: center;">
        <h4>Bouncing Balls</h4>
        <video controls width="100%">
          <source src="videos/bouncingballs.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <p>Toggling the timestamp in the GUI shows the balls bouncing, and the gaussian splats capture the nice shadows and squish of the balls.</p>
      </div>
      <div style="text-align: center;">
        <h4>Hell Warrior</h4>
        <video controls width="100%">
          <source src="videos/hellwarrior.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <p>Using the keycaps "I", "J", "K", and "L" is how you would rotate the camera around to see the warrior in the SIBR viewer.</p>
      </div>
      <div style="text-align: center;">
        <h4>Jumping Jacks</h4>
        <video controls width="100%">
          <source src="videos/jumpingjacks.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <p>Using the keycaps "W", "A", "S", and "D" is how you would move the camera up, down, and to the sides in the viewer.</p>
      </div>
      <h3>Language-Embedded Gaussians Interactive Viewer Visualizations</h3>
      <div style="text-align: center;">
        <h4>Feature Level 3 (Finest Grain, Subparts)</h4>
        <video controls width="100%">
          <source src="videos/legs_feat3.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <p>At our finest level all the subparts of the objects are segmented from each individual wing and part of the Gundamn to the buttons and joysticks on the controller and even the Uno marker on the stack of cards. </p>
      </div>
      <div style="text-align: center;">
        <h4>Feature Level 2 (Medium Grain, Parts)</h4>
        <video controls width="100%">
          <source src="videos/legs_feat2.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <p>At the second feature level we are able to segment out parts of large objects such as the hat or tie on Pikachu or the various components of the Gundam figure.</p>
      </div>
      <div style="text-align: center;">
        <h4>Feature Level 1 (Coarsest Grain, Whole Objects)</h4>
        <video controls width="100%">
          <source src="videos/legs_feat1.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <p>Here we demonstrate the trained Language-Embedded Gaussian at its coarsest grain giving us only each object with no further delineations. We also zoom very far in partway through the video to show how immersive our visualization is.</p>
      </div>
      <h3>LE4GS Interactive Viewer Visualization</h3>
      <div style="text-align: center;">
        <h4>Bouncing Balls</h4>
        <video controls width="100%">
          <source src="videos/le4gs.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <p>In this video we demonstrate the visualiztion of our trained LE4GS model. It is perfectly capable of supporting the 4D scene of the bouncing balls however when we swap to show the language features by pressing "Show LEGS Image" we notice that the viewer turns black. This is cause for every viewpoint the rendered image with language features is actuall null. We believe this is an issue with our training and are working on a fix.</p>
      </div>

    </section>
    <section id="in-progress-work" class="wrapper style3-alt fade-up">
      <div class='inner'>
        <h2>
          In-Progress Work
        </h2>
        <p>We have a lot of ideas that we're really interested in working on so we've listed them down below in no particular order!</p>
        <h3>Semantic Object Querying on the Interactive Viewer</h3>
        <p>Since we have an autoencoder that is trained off of all the language features in the scene is it possible for us to semantically query this semantic latent space and we can threshold our segmenation to only be the features of highest relevancy. We can then render the mask associated with this feature to get a segmentation of the object that we're querying! </p>
        <h3>Multi-Level Language Querying</h3>
        <p>The current implemenation of LangSplat handles semantic object segmentation by training three models at three different levels. We believe it should be possible to instead train one model using the language features of all three different models allowing for us to use one model for querying whole objects, its parts, or even its subparts.</p>
        <h3>Getting Language-Embedded 4D Gaussian Splats to Work</h3>
        <p>Our current issues with presently trained model are that the language features at any given point are zero. This results in the blank rendering that was shown in video above in the  We believe this due to an issue in how we're optimizing loss when fine-tuning as we currently still do optimize over the 4D Gaussian and the language features but it's possible that the weighting of language features for optimization are being diluted resulting in subpar results. We are investigating this more.</p>
        <h3>Training LE4GS on Photo-Realistic and Custom Data</h3>
        <p>Our approach is capable of working on photo-realistic scenes however most dynamic datasets collected in the real-world do not provide the ground truth camera poses. This means we much optimize these camera poses using Structure-from-Motion techniques such as COLMAP<sup>11,12</sup>. We are working through setting up COLMAP and have access to both iPhones 
          with LiDAR and Intel Realsense Depth cameras, meaning that we can capture both depth and RGB images. With these we're able to generate 
          custom dynamic scenes using real-world data. </p>
        <h3>Controller and VR Interactive Viewer</h3>
        <p>We found the interactive viewer while very cool to be a bit cumbersome in its controls. A bluetooth xbox controller would be a lot more intuitive for this and we're working on an implementation. Another thing that would be extremely interesting is rendering these gaussians in VR with the camera at any given point being your own eyes for immersive viewing.</p>
      </div>
    </section>
				<!-- References -->
					<section id="references" class="wrapper style4 fade-up">
						<div class="inner">
							<h2>References</h2>
							<ol>
								<li>Zeyu Yang, Hongye Yang, Zijie Pan, Li Zhang (2024). Real-time Photorealistic Dynamic Scene Representation and Rendering with 4D Gaussian Splatting. https://fudan-zvg.github.io/4d-gaussian-splatting/. </li>
								<li>Minghan Qin, Wanhua Li, Jiawei Zhou, Haoqian Wang, Hanspeter Pfister, Tsinghua University, Harvard University (2024). LangSplat: 3D Language Gaussian Splatting. https://langsplat.github.io/.</li>
								<li>Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng (2020). NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis. https://arxiv.org/pdf/2402.03307. </li>
								<li>Justin Kerr, Chung Min Kim, Ken Goldberg, Angjoo Kanazawa, Matthew Tancik (2023). LERF: Language Embedded Radiance Fields. https://www.lerf.io/. </li>
								<li>Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, George Drettakis (2023). 3D Gaussian Splatting for Real-Time Radiance Field Rendering. https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/. </li>
								<li>Yuanxing Duan, Fangyin Wei, Qiyu Dai, Yuhang He, Wenzheng Chen, Baoquan Chen (2024). 4D Gaussian Splatting: Towards Efficient Novel View Synthesis for Dynamic Scenes.</li>
								<li>Justin Yu, Kush Hari, Kishore Srinivas, Karim El-Refai, Adam Rashid, Chung Min Kim, Justin Kerr, Richard Cheng, Ashwin Balakrishna, Thomas Kollar, Ken Goldberg (2024). Incrementally Building Room-Scale Language-Embedded Gaussian Splats (LEGS) with a Mobile Robot. https://berkeleyautomation.github.io/LEGS/.</li>
								<li>Jonathon Luiten, Georgios kopanas, Bastian Leibe, Deva Ramanan (2023). Dynamic 3D Gaussians: Tracking by Persistent Dynamic View Synthesis. https://dynamic3dgaussians.github.io/.</li>
                <li>
                  https://grail.cs.washington.edu/rome/dense.html
                </li>
                <li>
                  https://huggingface.co/blog/gaussian-splatting
                </li>
                <li>Schönberger, J. L., Zheng, E., Pollefeys, M., & Frahm, J.-M. (2016). Pixelwise View Selection for Unstructured Multi-View Stereo. In European Conference on Computer Vision (ECCV).</li>
                <li>Schönberger, J. L., & Frahm, J.-M. (2016). Structure-from-Motion Revisited. In Conference on Computer Vision and Pattern Recognition (CVPR).</li>
							  </ol>
						</div>
					</section>

					<section id="contributions" class="wrapper style4-alt fade-up">
						<div class="inner">
							<h2>Contributions</h2>
							<h3>James</h3>
							<ul>
                <li> Paper Review: found and reviewed a number of recent and relevant papers on Gaussian Splatting, Dynamic Rendering, and Langage Embedded 3D scenes, helping to decide which research to use as our foundation to give the best results. 
                </li>
								<li>Resolution of Environment Issues: James played a crucial role in troubleshooting and resolving environment setup issues encountered at the beginning of the 4D Gaussian Splatting (4DGS) implementation. 
									His efforts ensured a smooth transition into the development phase of the project.</li>
								  <li>Integration of Models: spearheaded the integration of the 4DGS and LEGS models, combining their functionalities to create a unified framework for dynamic scene representation.</li>
								  <li>Theory Development: contributed to the theoretical underpinnings of the project, particularly in refining the conceptual framework behind the combined model approach.</li>
								  <li>Website Content Creation: wrote explanatory content for the project website of the project's goals, methodologies, and outcomes to a wider audience.</li>
							</ul>
							<h3>Karim</h3>
							<ul>
								<li>Brainstormed Theory Behind Combining Models: played a key role in conceptualizing how to integrate the 4D Gaussian Splatting (4DGS) and Language-Embedded 3D Gaussian Splatting (LEGS) models. 
									He provided the relevant research papers and synthesized the necessary theoretical framework to guide the team's efforts.</li>
								  <li>Research and Paper Review: conducted thorough research on various papers related to dynamic scene representation, object tracking, and semantic querying. He provided valuable insights from 
									these papers to inform the team's approach and decision-making process.</li>
								  <li>Model Training: took charge of training various models, including those for dynamic scenes like bouncing balls, hellwarrior, hook, and five more.</li>
								  <li>Setting Up Compute Devices: Karim was responsible for configuring and setting up the necessary compute devices required for training and running the models ("was stuck in CUDA hell"). His efforts ensured smooth execution 
									of computational tasks throughout the project.</li>
							</ul>
							<h3>Andrew</h3>
							<ul>
								<li>Presentation and Website Development: led the creation of the project presentation and webpage for the milestone. He meticulously organized and presented the project's progress, findings, and future 
									directions in a compelling and coherent manner.</li>
								  <li>Interactive Viewer Adaptation: adapted the interactive viewer to support 4DGS, enabling real-time visualization and manipulation of rendered scenes.</li>
								  <li>Visualization Creation:  created visualizations to showcase the capabilities of the interactive viewer and the rendered scenes.</li>
							</ul>
							<h3>Jenny</h3>
							<ul>
								<li>Presentation Development: contributed to the development of the project presentation.</li>
								<li>Website Content Creation: collaborated on writing content for the project webpage.</li>
                <li>Investigation of SIBR Viewer: Jenny explored the SIBR viewer, a crucial component for visualizing and interacting with the rendered scenes. He assessed its capabilities and compatibility with 
									the team's objectives, providing insights for its integration into the project workflow.</li>
							</ul>
						</div>
					</section>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
