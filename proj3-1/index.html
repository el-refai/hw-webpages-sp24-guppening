<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <style>
    body {
      background-color: #404040;
      background-color: white;
      padding: 100px;
      width: 1000px;
      margin: auto;
      text-align: left;
      font-weight: 300;
      font-family: 'Open Sans', sans-serif;
      color: #121212;
      cursor: url(images/bwunny_small.png), default!important;
      cursor: url(images/bwunny_small.png), pointer!important;
    }
    h1, h2, h3, h4 {
      font-family: 'Source Sans Pro', sans-serif;
    }
    kbd {
      color: #121212;
    }
    blockquote {
      color: #888;
      border: 2px solid #333;
      padding: 10px;
      background-color: #ccc;
    }

    table.custom-tbl {
      border: 1px solid;
    }

    table.custom-tbl th {
      border: 1px solid;
      background-color: rgb(99, 209, 209);
    }

    table.custom-tbl td {
      border: 1px solid;
      background-color: #f1e686a8;
    }

    /* The alert message box */
    .alert {
      padding: 20px;
      background-color: #f44336; /* Red */
      color: white;
      margin-bottom: 15px;
    }

    /* The close button */
    .closebtn {
      margin-left: 15px;
      color: white;
      font-weight: bold;
      float: right;
      font-size: 22px;
      line-height: 20px;
      cursor: pointer;
      transition: 0.3s;
    }

    /* When moving the mouse over the close button */
    .closebtn:hover {
      color: black;
    }
  </style>
<title>CS 184 Path Tracer</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Sans+Pro" rel="stylesheet">
<link rel="icon" href="./images/tream.png" type="image/x-icon">
<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']]
    }
  };
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>
<style type="text/css" media="screen">

	table{
	border-collapse:collapse;
	border:1px solid #000000;
	}
	
	table td{
	border:1px solid #000000;
	}

	table th{
	border:1px solid #000000;
	}
	</style>

<style>
	.centered-table {
		margin-left: -15%; /* Adjust the percentage to move the table more or less to the left */
	}
</style>
</head>


<body>

<h1 align="middle">CS 184: Computer Graphics and Imaging, Spring 2024</h1>
<h1 align="middle">Homework 3: Path Tracer</h1>
<h2 align="middle">James DeLoye and Karim El-Refai</h2>

<!-- Add Website URL -->
<h2 align="middle">Website URL <a href="https://el-refai.github.io/the_guppening/proj3-1/index.html">https://el-refai.github.io/the_guppening/proj3-1/index.html</a></h2>

<br><br>


<div align="center">
  <table style="width=100%">
      <tr>
          <td align="middle">
          <img src="images/bunny_background.webp" width="480px" />
          <figcaption align="middle">Results Caption: my bunny is the bounciest bunny</figcaption>
      </tr>
  </table>
</div>

<h2 align="middle">Overview</h2>
<p>
	In this project we implemented a ray tracer from the ground up. We started by implementing ray generation and sampling algorithms and moved onto intersections with primitive shapes like triangles and spheres. From there we significantly sped up intersection by building a bounding volume hierarchy (BVH), which is essentially a tree hierarchy of bounding boxes that allow for quick checks of whether a ray intersects any primitives in the box. Next, we implemented proper ray traced illumination with zero bounce illumination and then one bounce, first with hemisphere sampling and then with light sampling. Then we implemented global illumination, with support for up to $n$ bounces. We also used russian roulette to sample an unbounded number of bounces. Finally, we added adaptive sampling to focus rendering on the areas that took longest to converge, while saving computation in low variance area.

	In addition to all this, we implemented 7 extra credit components from the list, including GUI changes, jittered sampling, bilateral filtering, and even non-axis-aligned bounding boxes! Enjoy!
</p>
<br>

<h2 align="middle">Part 1: Ray Generation and Scene Intersection (20 Points)</h2>
<!-- Walk through the ray generation and primitive intersection parts of the rendering pipeline.
Explain the triangle intersection algorithm you implemented in your own words.
Show images with normal shading for a few small .dae files. -->

<h3>
  Walk through the ray generation and primitive intersection parts of the rendering pipeline.
</h3>
<p>
    To implement ray generation, we first had to think from the perspective of the camera looking at the world. Rays would come through the center of the camera and go out through its sensor, and out through the world. We had to convert these camera rays into rays in world coordinates so that they could easily be intersected with objects in the world. We did this using the camera to world transformation matrix to multiply the camera space coordinate that we computed. From here we added sampling, which for each pixel in our camera sensor sent a number of rays through random points in the pixel, and then the final pixel value was determined through an average of these values. Then we had to implement the logic that would actually allow these rays to intersect with triangles and spheres in our scenes, which determines what is actually rendered. This is detailed below:
</p>
<br>

<h3>
  Explain the triangle and sphere intersection algorithm you implemented in your own words.
</h3>
<p>
	For triangles, we adapted the Moller Trumbore algorithm from lecture to find the intersection of a ray within the triangle. The great part of this algorithm is that instead of generating the plane equation and calculating whether a ray intersects a triangle within the plane independently, the $\alpha$, $\beta$, and $\gamma$ values of the barycentric coordinates are calculated along with the intersection distance parameter of the ray, $t$. If the ray intersects within the triangle according to the barycentric coordinates (all $<1$) then the intersection is valid and we can perform the necessarily checks on the value of $t$ to see if the intersection is valid. In addition, with the barycentric coordinates found we can trivially find the normal at the intersection point by interpolating with the coordinates. 
</p>
<p>
	Spheres were even easier to implement. The intersection equation can be solved using the quadratic equation with $a$ value $a=d\cdot d$, $b$ value $2(o-c)\cdot d$, and $c$ value $(o-c)\cdot (o-c) - r^2$. We checked if the discriminant was positive and if so, we could find where there was an intersection, first checking the closer value and then the farther value to see if they were within our min_t and max_t bounds. 
</p>
<br>

<h3>
  Show images with normal shading for a few small .dae files.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/CBempty.png" align="middle" width="400px"/>
        <figcaption>CBempty</figcaption>
      </td>
      <td>
        <img src="images/spheres.png" align="middle" width="400px"/>
        <figcaption>CBspheres</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/banana.png" align="middle" width="400px"/>
        <figcaption>Banana</figcaption>
      </td>
      <td>
        <img src="images/CBcoil.png" align="middle" width="400px"/>
        <figcaption>CBcoil</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>


<h2 align="middle">Part 2: Bounding Volume Hierarchy (20 Points)</h2>
<!-- Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.
Show images with normal shading for a few large .dae files that you can only render with BVH acceleration.
Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration. Present your results in a one-paragraph analysis. -->

<h3>
  Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.
</h3>
<p>
    To construct the BVH, we started by getting the bounding box of all the primitives between the start and end iterators of the array. Our first heuristic for splitting was as follows: replicate the array of primitives and sort it along the X, Y, or Z axis. Then we take the median value, and then recursively call the build function on the first half and second half of the split array. At each level of recursion we would alternate between the X, Y, and Z axis. This was easy to implement and worked reasonably well, but was slow ($O(n \log n)$ at each level and $\log n$ levels in total giving a total runtime of $O(n \log^2{n})$). As a result we actually improved this to all be in place and done in $O(n \log n)$ time total, which is described below in the first extra credit item. However, we used this improved implementation for all of the following sections.
</p>

<h3>
  Show images with normal shading for a few large .dae files that you can only render with BVH acceleration.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/cow.png" align="middle" width="400px"/>
        <figcaption>cow: 5856 primitives</figcaption>
      </td>
      <td>
        <img src="images/building.png" align="middle" width="400px"/>
        <figcaption>building: 39506 primitives</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/maxplanck.png" align="middle" width="400px"/>
        <figcaption>maxplanck: 50801 primitives</figcaption>
      </td>
      <td>
        <img src="images/CBlucy.png" align="middle" width="400px"/>
        <figcaption>CBlucy: 133796 primitives</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<h3>
  Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration. Present your results in a one-paragraph analysis.
</h3>
<p>
	Below are our benchmarked results for rendering with and without the help of BVH acceleration. For super small files, like CBspheres, we can see that the overhead of traversing the BVH slows down the rendering speed. However, once the number of primitives gets up to thousands of polygons, like in teapot, banana, cow, and bunny, the naive primitive is hundreds to thousands of times slower. As the meshes get bigger, the BVH performs even better, showing the importance of the BVH in rendering complex meshes. 
</p>
<table align="center">
	<tr>
	  <th>DAE File</th>
	  <th>CBspheres_lambertian</th>
	  <th>Teapot</th>
	  <th>Banana</th>
	  <th>Cow</th>
	  <th>CBbunny</th>
	</tr>
	<tr>
	  <th>No BVH (million rays/s)</th>
	  <td>6.1921</td>
	  <td>0.0703</td>
	  <td>0.0688</td>
	  <td>0.0291</td>
	  <td>0.0044</td>
	</tr>
	<tr>
		<th>With BVH (million rays/s)</th>
		<td>4.9119</td>
		<td>3.5493</td>
		<td>4.6608</td>
		<td>3.6842</td>
		<td>3.3830</td>
	  </tr>
	<tr>
		<th>No BVH (render time (s))</th>
		<td>0.6154</td>
		<td>74.0937</td>
		<td>73.9594</td>
		<td>177.4888</td>
		<td>942.2607</td>
	  </tr>
	  <tr>
		<th>With BVH (render time (s))</th>
		<td>0.8182</td>
		<td>1.4048</td>
		<td>0.9076</td>
		<td>1.3801</td>
		<td>1.5110</td>
	  </tr>
  </table>
<br>

<h2 align="middle">Part 3: Direct Illumination (20 Points)</h2>
<!-- Walk through both implementations of the direct lighting function.
Show some images rendered with both implementations of the direct lighting function.
Focus on one particular scene with at least one area light and compare the noise levels in soft shadows when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling, not uniform hemisphere sampling.
Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis. -->

<h3>
  Walk through both implementations of the direct lighting function.
</h3>
<p>
    To implement direct lighting, we went through two methods: hemisphere sampling, and light importance sampling. Hemisphere sampling works by sampling light from a uniform incoming direction on the hemisphere. We used Monte-Carlo sampling to average these values out while taking the probability of choosing a specific direction into account. We check if the random ray intersects a light source, and then use the reflection equation to calculate how much light is reflected back towards the camera. Light importance sampling instead samples from the directions of lights and checks if the ray intersects something before hitting the light source. If it doesn't then we use the reflectance equation to calculate the light reflected back just like in the hemisphere sampling. 
</p>

<h3>
  Show some images rendered with both implementations of the direct lighting function.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <!-- Header -->
    <tr align="center">
      <th>
        <b>Uniform Hemisphere Sampling</b>
      </th>
      <th>
        <b>Light Sampling</b>
      </th>
    </tr>
    <br>
    <tr align="center">
      <td>
        <img src="images/CBspheres1.png" align="middle" width="400px"/>
        <figcaption>Hemisphere Sampled CBspheres_lambertian</figcaption>
      </td>
      <td>
        <img src="images/CBspheres_64_32.png" align="middle" width="400px"/>
        <figcaption>Light Sampled CBspheres_lambertian</figcaption>
      </td>
    </tr>
    <br>
    <tr align="center">
      <td>
        <img src="images/CBbunny_H_64_32.png" align="middle" width="400px"/>
        <figcaption>Hemisphere Sampled CBbunny</figcaption>
      </td>
      <td>
        <img src="images/CBbunny_64_32.png" align="middle" width="400px"/>
        <figcaption>Light Sampled CBbunny</figcaption>
      </td>
    </tr>
    <br>
  </table>
</div>
<br>

<h3>
  Focus on one particular scene with at least one area light and compare the noise levels in <b>soft shadows</b> when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling, <b>not</b> uniform hemisphere sampling.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/bunny_1_1_m1.png" align="middle" width="400px"/>
        <figcaption>1 Light Ray (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/bunny_1_4_m1.png" align="middle" width="400px"/>
        <figcaption>4 Light Rays (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/bunny_1_16_m1.png" align="middle" width="400px"/>
        <figcaption>16 Light Rays (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/bunny_1_64_m1.png" align="middle" width="400px"/>
        <figcaption>64 Light Rays (CBbunny.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<p>
    As the number of samples per light ray increases, the noise in the image decreases. This is because for each pixel the samples from each point get averaged out. So if a point is partially shrouded in shadow by a light source, then the shadow samples and light samples get averaged out to get the actual lighting value of that pixel. This is why the image gets smoother and the shadows get less noisy as the number of samples per area light increases. 
</p>
<br>

<h3>
  Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis.
</h3>
<p>
  Light importance sampling results in a much less noisy image as we are more specifically checking if there is an obstruction between the given point and light sources. Meanwhile, uniform hemisphere sampling is far noisier as a significant amount of the samples from uniform hemisphere sampling contribute little to no irradiance to the final pixel’s value. Importance sampling as a far lower sample rate is equivalent to uniform sampling at a higher sampling rate since they’ll both sample around the same number of meaningful directions.
</p>
<br>


<h2 align="middle">Part 4: Global Illumination (20 Points)</h2>
<!-- Walk through your implementation of the indirect lighting function.
Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.
Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination. Use 1024 samples per pixel. (You will have to edit PathTracer::at_least_one_bounce_radiance(...) in your code to generate these views.)
For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, and 100 (the -m flag). Use 1024 samples per pixel.
Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.
You will probably want to use the instructional machines for the above renders in order to not burn up your own computer for hours. -->

<h3>
  Walk through your implementation of the indirect lighting function.
</h3>
<p>
    To implement indirect lighting we need to be able to compute the light generated by 2+ bounces of light. We do this by realizing that n-bounce sampling is equivalent to sampling a ray and the (n-1)-bounce from the hit point this can then repeat recursively. We implement this with the following recursive approach: 
	<ol>
		<li> If depth of ray == 0 then we exit immediately as we’ve hit the max number of bounces	</li>
		<li> If depth of ray == 1 then we return the one_bounce_radiance of the ray	</li>
		<li> Otherwise, we set our Russian Roulette such that with probability 0.3 the ray terminates anyways</li>
		<li> If we’re in the 70% then we compute the next bounce of the ray by using the BSDF at the intersection point to give us a new sample direction. </li>
		<li> We then compute this new ray’s intersection with the scene and set this intersection’s depth to be one less than the previous ray’s depth. 	</li>
		<li> We recurse on this new ray and intersection	</li>
	   	<li> Compute the overall reflectance using the reflectance formula provided with the result of our recursion and add it to the total illumination so far.</li>
		<li>Return the total illumination</li>
	</ol>

</p>
<br>

<h3>
  Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/spheres_1024_32_full.png" align="middle" width="400px"/>
        <figcaption>CBspheres_lambertian.dae</figcaption>
      </td>
      <td>
        <img src="images/bunny_1024_32_full.png" align="middle" width="400px"/>
        <figcaption>CBbunny.dae</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<h3>
  Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination. Use 1024 samples per pixel. (You will have to edit PathTracer::at_least_one_bounce_radiance(...) in your code to generate these views.)
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/spheres_1024_32_1.png" align="middle" width="400px"/>
        <figcaption>Only direct illumination (CBspheres_lambertian.dae)</figcaption>
      </td>
      <td>
        <img src="images/spheres_1024_32.png" align="middle" width="400px"/>
        <figcaption>Only indirect illumination (CBspheres_lambertian.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p>
    Direct illumination results in very high contrast and unrealistic lighting scenarios as the shadows are totally black. This is because direct illumination uses only zero-bounce and one-bounce rays. Zero-bounce rays go directly from the light source to the camera while one-bounce rays only illuminate points that the light source directly reaches. Indirect illumination gives a far more realistic lighting scenario as with 2+ bounces the rays are now capable of illuminating points that the light source doesn’t directly hit. Giving us partially lit shadows and also illuminating the ceiling of the scene.
</p>
<br>

<h3>
	For CBbunny.dae, render the mth bounce of light with max_ray_depth set to 0, 1, 2, 3, 4, and 5 (the -m flag), and isAccumBounces=false. Explain in your writeup what you see for the 2nd and 3rd bounce of light, and how it contributes to the quality of the rendered image compared to rasterization. Use 1024 samples per pixel.
  </h3>

  <h3>
	For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, 4, and 5(the -m flag). Use 1024 samples per pixel.
  </h3>

  <table class="centered-table">
	<tr>
		<th>isAccumBounces = false</th>
		<td><img src="images/bunny_1024_8_m0_na.png" align="middle" width="200px"/></td>
		<td><img src="images/bunny_1024_8_m1_na.png" align="middle" width="200px"/></td>
		<td><img src="images/bunny_1024_8_m2_na.png" align="middle" width="200px"/></td>
		<td><img src="images/bunny_1024_8_m3_na.png" align="middle" width="200px"/></td>
		<td><img src="images/bunny_1024_8_m4_na.png" align="middle" width="200px"/></td>
		<td><img src="images/bunny_1024_8_m5_na.png" align="middle" width="200px"/></td>
	  </tr>
	<tr>
		<th>isAccumBounces = true</th>
		<td><img src="images/bunny_1024_8_m0_a.png" align="middle" width="200px"/></td>
		<td><img src="images/bunny_1024_8_m1_a.png" align="middle" width="200px"/></td>
		<td><img src="images/bunny_1024_8_m2_a.png" align="middle" width="200px"/></td>
		<td><img src="images/bunny_1024_8_m3_a.png" align="middle" width="200px"/></td>
		<td><img src="images/bunny_1024_8_m4_a.png" align="middle" width="200px"/></td>
		<td><img src="images/bunny_1024_8_m5_a.png" align="middle" width="200px"/></td>
	  </tr>
	</table>
	  </table>


  <br>
  <p>
	The second and third bounce of light both represent indirect lighting. We see that the lighting provided by the second bounce is a lot larger than the third bounce however the third bounce is more subtle lighting as evidenced by it illuminating the edges of the room and bunny relatively more so than any other part of the scene. These two examples show how rendering images with ray-tracing provides more realistic and balanced lighting compared to rasterization as rasterization does not pick up these subtle differences in lighting. 

When we set the max ray depth to 0 we get the same result as zero-bounce lighting and if we set max ray depth to 1 we get the same result as one-bounce lighting. At 0 depth we only have the illuminated light source and at 1 depth only surfaces in direct line of sight of the light source are illuminated with everything else being completely unilluminated. When .

  </p>
  <br>


<h3>
	For CBbunny.dae, output the Russian Roulette rendering with max_ray_depth set to 0, 1, 2, 3, 4, and 100(the -m flag). Use 1024 samples per pixel.	
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/bunny_1024_32_m0.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 0 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/bunny_1024_32_m1.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 1 (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/bunny_1024_32_m2.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 2 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/bunny_1024_32_m3.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 3 (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/bunny_1024_32_m100.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 100 (CBbunny.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p>
    We see that Russian Roulette rendering does not result in a particularly noticeable decrease in quality of our renders and also runs much faster!  
</p>
<br>

<h3>
  Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/bunny_1_4_.png" align="middle" width="400px"/>
        <figcaption>1 sample per pixel (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/bunny_2_4_.png" align="middle" width="400px"/>
        <figcaption>2 samples per pixel (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/bunny_4_4_.png" align="middle" width="400px"/>
        <figcaption>4 samples per pixel (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/bunny_8_4_.png" align="middle" width="400px"/>
        <figcaption>8 samples per pixel (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/bunny_16_4_.png" align="middle" width="400px"/>
        <figcaption>16 samples per pixel (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/bunny_64_4_.png" align="middle" width="400px"/>
        <figcaption>64 samples per pixel (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/bunny_1024_4_.png" align="middle" width="400px"/>
        <figcaption>1024 samples per pixel (CBbunny.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p>
    Increasing the sample-per-pixel rates does result in less noisy renders however even with a sample-per-pixel rate it still has a noticeable amount of noise. The fact that as sample-per-pixel rate increases, noise decreases is due to our Monte-Carlo estimator as the fewer samples we take of a pixel results in less accurate estimations of the illuminance from our light-source, giving us pixels that are brighter/darker than they should be (noise). In order to get fully noiseless images we need to increase the sample-per-pixel rate a lot, all the way to 1024.
</p>
<br>


<h2 align="middle">Part 5: Adaptive Sampling (20 Points)</h2>
<!-- Explain adaptive sampling. Walk through your implementation of the adaptive sampling.
Pick one scene and render it with at least 2048 samples per pixel. Show a good sampling rate image with clearly visible differences in sampling rate over various regions and pixels. Include both your sample rate image, which shows your how your adaptive sampling changes depending on which part of the image you are rendering, and your noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth. -->

<h3>
  Explain adaptive sampling. Walk through your implementation of the adaptive sampling.
</h3>
<p>
    We saw from above that we can get noiseless images if we increase the sample-per-pixel rate high enough. Unfortunately, as we increase the sample rate computation time quickly increases as well. This is why instead of increasing our sampling rate by the same amount for all pixels we use adaptive sampling to vary our sampling rate per pixel by sampling pixels that converge quicker less and vice versa for pixels that converge slower. Adaptive sampling works by computing $I$ where $I = 1.95 \cdot \frac{\sigma}{\sqrt{n}}$ where $\sigma$ is the standard deviation of the pixel and $n$ is the number of samples you’ve already traced through this pixel. So $I$ is small only when the variance is small or when the number of samples $n$ is large enough. This means for a small $I$ we can be more confident that the pixel has converged so we check if $I \leq maxTolerance \cdot \mu$ where $\mu$ is the mean of the pixel and $maxTolerance$ is a hyperparameter we tune but is $0.05$ for this purpose. We implemented adaptive sampling by having two variables $s_1$ and $s_2$ where $s_1$ stores the sum of the illuminance for this given ray so far and $s_2$ stores the sum of the squared illuminance for each ray. Whenever we take a new sample we update these variables and every $samplesPerBatch$ samples we recalculate $I$ to see if it’s $\leq maxTolerance \cdot \mu$. Where $\mu = \frac{s_1}{n}$ and $\sigma^2 = \frac{1}{n-1} \cdot \left( s_2 - \frac{s_1^2}{n}\right)$.
</p>
<br>

<h3>
  Pick two scenes and render them with at least 2048 samples per pixel. Show a good sampling rate image with clearly visible differences in sampling rate over various regions and pixels. Include both your sample rate image, which shows your how your adaptive sampling changes depending on which part of the image you are rendering, and your noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/spheres_2048_1_m5.png" align="middle" width="400px"/>
        <figcaption>Rendered image (CBspheres.dae)</figcaption>
      </td>
      <td>
        <img src="images/spheres_2048_1_m5_rate.png" align="middle" width="400px"/>
        <figcaption>Sample rate image (CBspheres.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/bunny_2048_1_m5.png" align="middle" width="400px"/>
        <figcaption>Rendered image (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/bunny_2048_1_m5_rate.png" align="middle" width="400px"/>
        <figcaption>Sample rate image (CBbunny.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<h1 align="middle">Extra Credit Compendium</h1>

<h3 align="middle">Because of all the possible extra credit opportunities, we took it as a challenge: how many can we implement?</h3>

<h2 align="middle">Extra Credit 1: Memory Efficient BVH Generation</h2>
<p>
	As opposed to our original BVH generation strategy which employed sorts and copies of the primitive array, we instead looked to build the BVH without any memory copying at all. To do this, we started by changing our splitting heuristic, and after some reading settled on taking the centroid of the bounding box of primitive's centroids. We then split along the largest axis. To do this, we took inspiration from the quickselect median finding algorithm to partition the primitives array in place, which can be done in $O(n)$ time. Just like before, we then recursively called the function. This resulted in a much faster BVH generation time, with an average time of $O(n \log n)$, and a worst case time of $O(n^2)$, in the case that the partitions are very unbalanced. Below are some benchmarks of the in-place vs the original BVH generation strategy.
</p>

<table align="center">
	<tr>
	  <th>DAE File</th>
	  <th>Teapot</th>
	  <th>Cow</th>
	  <th>Building</th>
	  <th>Maxplanck</th>
	  <th>CBlucy</th>
	</tr>
	<tr>
	  <th>Inefficient BVH Generation Time (s)</th>
	  <td>0.0026</td>
	  <td>0.0068</td>
	  <td>0.0703</td>
	  <td>0.0968</td>
	  <td>0.3246</td>
	</tr>
	<tr>
		<th>Memory Efficient BVH Generation Time (s)</th>
		<td>0.0008</td>
		<td>0.0018</td>
		<td>0.0185</td>
		<td>0.0223</td>
		<td>0.0736</td>
	  </tr>
  </table>
<br>

<p>
	As can be seen from the above, the memory efficient strategy is consistently 2.5-4 times faster than the original strategy. It also has the added benefit of saving significant memory, compared to the $O(n \log n)$ memory usage of the original strategy.
</p>

<h2 align="middle">Extra Credit 2: Iterative BVH Traversal</h2>
<p> 
	To implement the iterative BVH traversal, we essentially did a depth-first search with the std::stack class. This was simple to implement, and we did it in the BVH construction, and BVH intersection functions. From there we benchmarked these iterative traversals against the recursive ones. Below are the results of these benchmarks.
</p>

<table align="center">
	<tr>
		<th>DAE File</th>
		<th>Teapot</th>
		<th>Cow</th>
		<th>Building</th>
		<th>Maxplanck</th>
		<th>CBlucy</th>
	  </tr>
	  <tr>
		<th>Recursive BVH Generation Time (s)</th>
		<td>0.0008</td>
		<td>0.0017</td>
		<td>0.0195</td>
		<td>0.0234</td>
		<td>0.0736</td>
	  </tr>
	  <tr>
		  <th>Iterative BVH Generation Time (s)</th>
		  <td>0.0007</td>
		  <td>0.0017</td>
		  <td>0.0191</td>
		  <td>0.0222</td>
		  <td>0.0733</td>
		</tr>
	<tr>
	  <th>Recursive Intersection (million rays/s)</th>
	  <td>3.5476</td>
	  <td>3.6802</td>
	  <td>7.1397</td>
	  <td>2.6546</td>
	  <td>2.6346</td>
	</tr>
	<tr>
		<th>Iterative Intersection (million rays/s)</th>
		<td>3.5565</td>
		<td>3.6759</td>
		<td>6.8987</td>
		<td>2.6558</td>
		<td>2.6566</td>
	  </tr>
	<tr>
		<th>Recursive Intersection (render time (s))</th>
		<td>1.4037</td>
		<td>1.3817</td>
		<td>0.5527</td>
		<td>1.9453</td>
		<td>1.9651</td>
	  </tr>
	  <tr>
		<th>Iterative Intersection (render time (s))</th>
		<td>1.4018</td>
		<td>1.3843</td>
		<td>0.5803</td>
		<td>1.9456</td>
		<td>1.9562</td>
	  </tr>
  </table>
<br>

<p>
	As can be seen from the above, surprisingly, the iterative BVH traversal is not faster (within margin of error) than the recursive one in terms of construction or overall render time. This is likely due to two reasons: the first is that recursive traversals is already highly optimized by the C++ compiler, and as a result there is not significant overhead. Second, the iterative traversal has its own overhead with respect to the stack operations, which may counter any speedup from the iterative algorithm itself. That said, the iterative traversal is still a useful tool, as it can save stack memory versus deep recursion. 
</p>

<h2 align="middle">Extra Credit 3: GUI Addition</h2>

<p>
	When coming up with ideas of what to add to the GUI, we were inspired by the potential of being able to see higher ray depth renders superimposed upon lower ray depth renders. To this end, we bound the 'u' key to be like the 'c' key, and activate cell rendering on a subset of the pixels, except that for this subset, they render the pixels at one max ray depth higher than the rest of the pixels. As a result, you can see what the effects of the additional ray depth is on the lighting of certain areas, particularly in the shadows. Below are some examples of this in action (albeit at a relatively low sampling rate to save rendering time).
</p>

<div align="middle">
	<table style="width:100%">
	  <tr align="center">
		<td>
		  <img src="images/CBspheres_lambertian_screenshot_3-16_12-43-36.png" align="middle" width="400px"/>
		  <figcaption>Adding direct illumination to certain areas of the scene (CBspheres_lambertian.dae)</figcaption>
		</td>
		<td>
		  <img src="images/CBspheres_lambertian_screenshot_3-16_12-47-25.png" align="middle" width="400px"/>
		  <figcaption>Adding indirect illumination to shadows (CBspheres_lambertian.dae)</figcaption>
		</td>
	  </tr>
	</table>
  </div>
<h2 align="middle">Extra Credit 4: Jittered Ray Sampler</h2>
<p>
	To create the jittered sampler, we began by making a new sampler class. When initialized, this sampler takes in the lowest number of samples that a pixel could be sampled for (samples per batch in the case of adaptive sampling) and then sets the number of subpixels per pixel grid to the smallest perfect square less than that. It also stores a state that tells which subpixel its currently jitter sampling for. Each time this jitter sampler is called, it figures out which subpixel its on and then returns a random point within that subpixel, thus implementing the jittered behavior. Below are some examples of the jittered sampler in action, compared to the regular fully random sampler.
</p>

<div align="middle">
  <table>
	<tr align="center">
	  <td>
		<img src="images/dwagon1.png" align="middle" width="400px"/>
    <figcaption>Original Random Sampler</figcaption>
	  </td>
	  <td>
		<img src="images/dwagon2.png" align="middle" width="400px"/>
    <figcaption>Jittered Sampler</figcaption>
	  </td>
	</tr>
  </table>
  <p>We notice that difference between the random sampler and the jittered sampler in terms of produced images is quite minor. One noticeable thing though is the difference in sharpness between the shadows on the scales of the dragon. In the jittered sampler the shadows are softer while it is more pronounced on the random sampler.</p>
<<<<<<< HEAD
</div>
=======
>>>>>>> d343eafbce6078487033c5da536eb4a48ba1fe33

<h2 align="middle">Extra Credit 5: Surface Area Heuristic</h2>
<p>
	Our implementation of the surface area heuristic was guided by the slides and the description in the textbook <a href="https://www.pbr-book.org/3ed-2018/Primitives_and_Intersection_Acceleration/Bounding_Volume_Hierarchies#TheSurfaceAreaHeuristic">here</a>. To this end, we make a 2d array that is size 3x(num_buckets) so that we can partition the objects in to buckets in each of the three dimensions. We then put the objects into buckets based on their relative offset within the overall bounding box. Finally, we go through all possible splits of the buckets, and use the surface area heuristic of the resulting sub-bounding-boxes to find the minimum cost split according to the equation $C = \frac 18 + (\#l*\text{left_sa}) +  (\#r*\text{right_sa})$. However, if this split is still more costly than just making the node a leaf, then we instead make a it a leaf with all the primitives. Below are some benchmarks comparing the surface area heuristic to the original splitting heuristic. 
</p>

	Our implementation of the surface area heuristic was guided by the slides and the description in the textbook <a href="https://www.pbr-book.org/3ed-2018/Primitives_and_Intersection_Acceleration/Bounding_Volume_Hierarchies#TheSurfaceAreaHeuristic">here</a>. To this end, we make a 2d array that is size 3x(num_buckets) so that we can partition the objects in to buckets in each of the three dimensions. We then put the objects into buckets based on their relative offset within the overall bounding box. Finally, we go through all possible splits of the buckets, and use the surface area heuristic of the resulting sub-bounding-boxes to find the minimum cost split according to the equation $C = \frac 18 + (\#l*\text{left_sa}) +  (\#r*\text{right_sa})$. However, if this split is still more costly than just making the node a leaf, then we instead make a it a leaf with all the primitives. Below are some benchmarks comparing the surface area heuristic to the original splitting heuristic on dragon.dae and cow.dae.
</p>

<table align="center">
	<tr>
	  <th></th>
	  <th>Original</th>
	  <th> SAH 16 Buckets</th>
    <th> SAH 64 Buckets</th>
    <th> SAH 256 Buckets</th>
	</tr>
	<tr>
	  <th>BVH Generation Time (s)</th>
	  <td>0.0795</td>
    <td>0.3319</td>
    <td>0.9942</td>
    <td>15.7861</td>
	</tr>
	<tr>
		<th>Render Time (s)</th>
		<td>6.3114</td>
    <td>6.0666s</td>
    <td>3.3439</td>
    <td>4.9251</td>
	  </tr>
    <tr>
      <th> Average Speed (million rays/s)</th>
      <td>2.4067 </td>
      <td>2.3925 </td>
      <td>3.4980</td>
      <td>2.3981</td>
      </tr>
	  <tr>
		<th> Avg. Intersection Tests per Ray</th>
		<td>10.754120 </td>
    <td>11.970653 </td>
    <td>11.567262</td>
    <td>11.559629</td>
	  </tr>
  </table>
  <figcaption>Results on dragon.dae</figcaption>
  <table align="center">
    <tr>
      <th></th>
      <th>Original</th>
      <th> SAH 16 Buckets</th>
      <th> SAH 64 Buckets</th>
      <th> SAH 256 Buckets</th>
    </tr>
    <tr>
      <th>BVH Generation Time (s)</th>
      <td>0.0011</td>
      <td>0.0070</td>
      <td>0.0629</td>
      <td>1.0230</td>
    </tr>
    <tr>
      <th>Render Time (s)</th>
      <td>1.6559</td>
      <td>2.6614</td>
      <td>2.8052</td>
      <td>2.4061</td>
      </tr>
      <tr>
        <th> Average Speed (million rays/s)</th>
        <td>2.6482</td>
        <td>2.6872</td>
        <td>2.4799</td>
        <td>2.5077</td>
        </tr>
      <tr>
      <th> Avg. Intersection Tests per Ray</th>
      <td>11.408705</td>
      <td>10.790879</td>
      <td>10.277064</td>
      <td>10.596324</td>
      </tr>
    </table>
    <figcaption>Results on cow.dae</figcaption>
  
  <p>
	Above, we compare the SAH performance against the improved heuristic that was detailed in EC part 1. As can be seen, the SAH already incurs significant overhead in terms of BVH generation time, and this increases as the number of buckets does. However, the render time is significantly faster for the SAH, due to its more intelligent splitting model. The smaller number of intersections per ray as compared to the centroid heuristic is likely due to the better binning of the primitives. However, its is notable that by increasing the number of buckets past a certain point, there are diminishing returns, as can be seen when the number of buckets is increases from 64 to 256, where the values are within run-to-run variance. Overall though, the SAH is a solid improvement over simpler heuristics and presents a good tradeoff between BVH generation time and render time.
  </p>

<h2 align="middle">Extra Credit 6: Bilateral Denoising</h2>
<p> To implement bilateral denoising, we had to do significantly more research and reading into the codebase than for the previous extra credits. We started by reading through the code and found where we could post-process the image. After all threads were done processing (were in the DONE state), we called the bilateral_filter() function which was a property of the image class, which would apply the bilateral filter accordingly. The bilateral filter itself was implemented according to the original paper on bilateral filtering, which can be found <a href="https://users.soe.ucsc.edu/~manduchi/Papers/ICCV98.pdf">here</a>. Essentially, it runs a special 2D convolution on the image, where the kernel is a function of the distance between the pixels and the intensity of the pixels. For example, the weight for pixel $(i, j)$ from pixel $(k, l)$ is $w(i, j, k, l) = exp(-\frac{(i-k)^2 + (j-l)^2}{2\sigma_d^2} -\frac{(I(i, j) - I(k, l))^2}{2\sigma_r^2})$, where $I$ is the intensity value of the pixel. We then use this weight to compute the new intensity value of each pixel. This can be used to create a filter that smooths the image, while preserving edges. Below are some examples of the bilateral filter in action for varying values of $\sigma_d$ and $\sigma_r$.
</p>
  <div align="middle">
    <table>
    <tr align="center">
      <tr align="center">
        <td>
          <img src="images/dwagon5.png" align="middle" width="400px"/>
          <figcaption>Original</figcaption>
        </td>
      <td>
        <img src="images/dwagon_1_300.png" align="middle" width="400px"/>
        <figcaption>$\sigma_d = 1, \sigma_r = 300$</figcaption>
      </td>
      <td>
        <img src="images/dwagon_3_300.png" align="middle" width="400px"/>
        <figcaption>$\sigma_d = 3, \sigma_r = 300$</figcaption>
      </td>
      </tr>
    </table>
    </div>

    <div align="middle">
      <table style="width:100%">
        <tr align="center">
          <td>
            <img src="images/dwagon_3_10.png" align="middle" width="400px"/>
            <figcaption>$\sigma_d = 3, \sigma_r = 10$</figcaption>
          </td>
          <td>
            <img src="images/dwagon_3_30.png" align="middle" width="400px"/>
            <figcaption>$\sigma_d = 3, \sigma_r = 30$</figcaption>
          </td>
          <td>
            <img src="images/dwagon_10_300.png" align="middle" width="400px"/>
            <figcaption>$\sigma_d = 10, \sigma_r = 300$</figcaption>
          </td>
        </tr>
      </table>
    </div>

	<p> 
		As can be seen from the images the bilateral filter is a powerful tool for blurring the image and denoising scenes while maintaining the edges in the scene. Interestingly, we don't see as much a difference with changes in the $\sigma_r$ parameter, likely due to that this is based on the intensity of the pixel, and in the dragon image most pixels, save for those on the edges, are extremely close in color and thus intensity. As a result, varying the $\sigma_d$ parameter has a much more noticeable effect on the image, and thus can be effective at denoising the image or purposely applying a blur to it. Particularly in the second image with $\sigma_d = 1, \sigma_r = 300$, we see that the image is significantly smoother without losing too much detail, whereas in the unfiltered image the noise is still quite visible. Another parameter that can further change the look is the kernel size, which we limited to 5x5. However, larger kernels coupled with larger $\sigma$ values can result in a more pronounced blur.
	</p>

<h2 align="middle">Extra Credit 7: Non-Axis-Aligned Bounding Boxes</h2>
<p>
	And now, the crown jewel of our extra credits: non-axis-aligned bounding boxes. To implement this, we had to change a number of things in our project, from how the BVH was made to making a new BBox class, all while understanding semi-complex computational geometry algorithms. To begin, we read up on what non-axis-aligned bounding boxes even were and why they were useful. Their primary advantage is that by being non-axis-aligned, they can more tightly fit the geometry, and as a result we can get the minimum-volume bounding box. When investigating minimum-volume bounding box algorithms, we learned about O'Rourke's algorithm, which is a variation of the rotating calipers algorithm. All minimum-volume bounding box algorithms start with finding the convex hull of the points, so we found an off the shelf convex hull algorithm to use, and settled on the quickhull algorithm, which is a $O(n \log n)$ time algorithm. We used the code from<a href="https://github.com/akuukka/quickhull"> this repository</a> due to its portability and compatibility with our codebase. However, there was one issue: to get a convex hull we needed a pointcloud, so we had to create a new function for primitives called get_pointcloud, which would return a vector of points that were on the surface of the primitive. For triangles this was simple, but for spheres not so much. We settled on using an icosahedron to approximate the sphere, since it only had twelve points and was easy to calculate the icosahedron which has a sphere inscribed. After we had the convex hull, we then used O'Rourke's algorithm to find the minimum-volume bounding box. We primarily used the resources from the <a href="https://link.springer.com/content/pdf/10.1007/bf00991005.pdf">original paper</a> and <a href="https://www.cs.swarthmore.edu/~adanner/cs97/s08/pdf/calipers.pdf">this paper</a> on rotating calipers in order to implement the remainder of the algorithm. Most simply, the algorithm operates by rotating the convex hull about the x and z axes and finding the max of all values and min of all values as we would with an axis-aligned box. This is equivalent to finding all possible rotations of the points, and among these we take the orientation that results in the minimum axis-aligned box. We then rotate the points and the box back to their original position, but store the vertices of the box and the rotation matrix. In theory this second part is $O(n)$ since the number of rotations tried is a discrete and constant amount. Ultimately though the constant factors on this $O(n)$ are very high, to the point where it often takes longer than the $O(n \log n)$ convex hull operation which has lower constant factors. Ultimately though, bounding box generation is now an $O(n \log n)$ operation whereas it was $O(1)$ before, which is a significant slowdown during BVH generation. To intersect this new box, we implemented an intersection function that would rotate the ray and the box back to the axis-aligned frame, and then intersect the box normally. Since this was an orthonormal rotation, the distances would be preserved and we could use the same t values for the intersection. In addition, we implemented a new draw function for the non-axis-aligned bounding box class that would allow for these new boxes to be visualized. The non-axis-aligned boxes and their performance as compared to axis-aligned boxes can be seen below. Note that the same splitting heuristic was used for both the axis-aligned and non-axis-aligned boxes, and that the only difference is that here we use the minimum volume non-axis-aligned bounding boxes. 
</p>

<div align="middle">
  <table>
	<tr align="center">
		<tr align="center">
			<th>
			  Aligned Bounding Box
			</th>
			<th>
			  Non-Aligned Bounding Box
			</th>
		  </tr>
		<td>
		  <img src="images/alignedspheres.png" align="middle" width="400px"/>
		</td>
		<td>
		  <img src="images/nonalignedspheres.png" align="middle" width="400px"/>
		</td>
	  </tr>
	  <tr align="center">
		<td>
		  <img src="images/alignedteapot.png" align="middle" width="400px"/>
		</td>
		<td>
		  <img src="images/nonalignedteapot.png" align="middle" width="400px"/>
		</td>
	  </tr>
	  <tr align="center">
		<td>
		  <img src="images/alignedspout.png" align="middle" width="400px"/>
		</td>
		<td>
		  <img src="images/nonalignedspout.png" align="middle" width="400px"/>
		</td>
	  </tr>
	<tr align="center">
	  <td>
		<img src="images/alignedcow.png" align="middle" width="400px"/>
	  </td>
	  <td>
		<img src="images/nonalignedcow.png" align="middle" width="400px"/>
	  </td>
	</tr>
  </table>

  <table align="center">
    <tr>
      <th></th>
      <th>Cow.dae (Axis-Aligned)</th>
      <th>Cow.dae (Non-Axis-Aligned)</th>
      <th> Teapot.dae (Axis-Aligned)</th>
      <th> Teapot.dae (Non-Axis-Aligned)</th>
    </tr>
    <tr>
      <th>BVH Generation Time (s)</th>
      <td>0.0009</td>
      <td>12.2327</td>
      <td>0.0004</td>
      <td>5.1117</td>
    </tr>
    <tr>
      <th>Render Time (s)</th>
      <td>1.6160</td>
      <td>1.8934</td>
      <td>0.9376</td>
      <td>1.1780</td>
      </tr>
      <tr>
        <th>Avg. Speed (million rays/s)</th>
        <td>3.1496</td>
        <td>4.2539</td>
        <td>5.6229</td>
        <td>4.8027 </td>
        </tr>
      <tr>
      <th>Avg. Intersection Tests per Ray</th>
      <td>8.431184</td>
      <td>4.616134 </td>
      <td>6.421805</td>
      <td>4.645373 </td>
      </tr>
    </table>
    <p>We find that BVH generation with non-axis-aligned bounding boxes is significantly slower than with axis-aligned bounding boxes. However the number of intersection tests per ray is noticeably lower for both scenes. With the decrease in intersection tests growing proportional to the number of primitives in the scene as evidenced by it being nearly half between non-axis-aligned vs. axis-aligned. Initially, with small primitive scenes like the teapot, non-axis-aligned renders slower but we find non-axis-aligned actually taking over when moving to the more complex scene using cow.dae. This shows that non-axis-aligned bounding boxes are very promising for very large scale scenes where this reduction in intersection tests per ray becomes more pivotal.</p>
</body>
</html>
